import os
import time
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain_classic.text_splitter import RecursiveCharacterTextSplitter
from langchain_classic.chains import RetrievalQA

from app.core.config import settings
from app.infrastructure.llm_client import LLMClient

class RAGService:
    def __init__(self):
        self.doc_path = settings.KNOWLEDGE_BASE_PATH
        self.llm = LLMClient.get_llm()
        self.embeddings = LLMClient.get_embeddings()
        self.vector_store = None
        
        self._init_knowledge_base()

    def _init_knowledge_base(self):
        if not os.path.exists(self.doc_path):
            print(f"‚ö†Ô∏è RAG Service: Base n√£o encontrada em {self.doc_path}")
            return

        try:
            print("üîÑ Iniciando processamento da Base de Conhecimento...")
            loader = TextLoader(self.doc_path, encoding="utf-8")
            documents = loader.load()
            
            # O Recursive tenta quebrar primeiro por se√ß√µes (##), depois par√°grafos.
            # Isso garante que n√£o tenhamos chunks de 26.000 caracteres.
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=100,
                separators=["\n## ", "\n\n", "\n", " ", ""]
            )
            texts = splitter.split_documents(documents)
            print(f"üìä Documento quebrado em {len(texts)} fragmentos.")

            # Se tentarmos indexar tudo de uma vez, tomamos erro 429.
            # Vamos indexar de 30 em 30 e pausar um pouco.
            
            batch_size = 30
            total_batches = len(texts) // batch_size + 1
            
            # Cria o banco com o primeiro lote para inicializar
            if texts:
                print("üöÄ Criando √≠ndice vetorial (pode demorar uns segundos)...")
                first_batch = texts[:batch_size]
                self.vector_store = FAISS.from_documents(first_batch, self.embeddings)
                
                # Adiciona o resto em loop com pausa
                for i in range(batch_size, len(texts), batch_size):
                    batch = texts[i : i + batch_size]
                    if batch:
                        self.vector_store.add_documents(batch)
                        print(f"   ‚è≥ Processando lote {i//batch_size + 1}/{total_batches}...", end="\r")
                        time.sleep(1.5) # Pausa estrat√©gica para a API respirar

            print("\n‚úÖ RAG Service: Indexa√ß√£o conclu√≠da com sucesso!")
            
        except Exception as e:
            print(f"\n‚ùå RAG Service Erro Cr√≠tico: {e}")

    def ask(self, question: str) -> str:
        if not self.vector_store:
            return "Erro: O sistema est√° recarregando a base de conhecimento ou ela est√° offline."

        try:
            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                # Traz 4 contextos
                retriever=self.vector_store.as_retriever(search_kwargs={"k": 4}) 
            )
            return qa_chain.invoke({"query": question})["result"]
            
        except Exception as e:
            return f"Erro ao processar: {str(e)}"

rag_service = RAGService()